{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Conv2D, Flatten, MaxPooling2D, Reshape, UpSampling2D, BatchNormalization, AveragePooling2D, Activation, Lambda, Conv2DTranspose, ReLU\n",
    "from keras.models import Model, Sequential, load_model\n",
    "from keras.regularizers import l1\n",
    "\n",
    "in_height = 720//2\n",
    "in_width =  1280//2\n",
    "\n",
    "#out_height = 720\n",
    "#out_width = 1280\n",
    "\n",
    "out_height = in_height\n",
    "out_width = in_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.constraints import maxnorm\n",
    "from keras.losses import *\n",
    "encoder_input =  Input(shape=(in_height,in_width,3))\n",
    "\n",
    "# Encoder Layers\n",
    "encoder = Conv2D(8, (3, 3),padding='same', kernel_constraint=maxnorm(3))(encoder_input)\n",
    "encoder = Activation('relu')(encoder)\n",
    "encoder = AveragePooling2D((2, 2), padding='same',)(encoder)\n",
    "encoder = Conv2D(16, (3, 3), padding='same', kernel_constraint=maxnorm(3))(encoder)\n",
    "encoder = Activation('relu')(encoder)\n",
    "encoder = AveragePooling2D((2, 2), padding='same')(encoder)\n",
    "\n",
    "encoder = Conv2D(16, (3, 3), padding='same', kernel_constraint=maxnorm(3))(encoder)\n",
    "encoder = Activation('relu')(encoder)\n",
    "encoder = AveragePooling2D((2, 2), padding='same')(encoder)\n",
    "\n",
    "encoder =  Flatten(name='flatten_1')(encoder)\n",
    "\n",
    "encoder_model = Model(encoder_input,encoder)\n",
    "encoder_model.summary()\n",
    "\n",
    "encoder_model.load_weights(\"16C-360x640-encoder.h5\")\n",
    "\n",
    "# Decoder Layers\n",
    "decoder_input = Input(shape=(45 * 80 * 16,))\n",
    "decoder = Reshape(( 45, 80, 16))(decoder_input)\n",
    "decoder = Conv2DTranspose(128, (3, 3), padding='same',strides=2, kernel_constraint=maxnorm(3))(decoder)\n",
    "decoder = Activation('relu')(decoder)\n",
    "\n",
    "decoder = Conv2DTranspose(64, (3, 3), padding='same',strides=2, kernel_constraint=maxnorm(3))(decoder)\n",
    "decoder = Activation('relu')(decoder)\n",
    "\n",
    "decoder = Conv2DTranspose(32, (3, 3), padding='same',strides=2, kernel_constraint=maxnorm(3))(decoder)\n",
    "\n",
    "decoder = Conv2DTranspose(3, (1, 1), padding='same')(decoder)\n",
    "\n",
    "#decoder = ReLU(max_value=255)(decoder)\n",
    "decoder_model = Model(decoder_input,decoder)\n",
    "decoder_model.summary()\n",
    "\n",
    "autoencoder = Model(encoder_input,decoder_model(encoder))\n",
    "autoencoder.compile(optimizer='adam',  loss=logcosh,metrics=['acc'])\n",
    "\n",
    "autoencoder.load_weights('16C-Text--Images-autoencoder-weights-model-13-360x640-360x640.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = load_model(\"./autoencoder-model-11-360x640-720x1280.h5\")\n",
    "autoencoder.compile(optimizer='adam', loss='mean_absolute_error',metrics=['acc'])\n",
    "\n",
    "encoder_model = Model(autoencoder.input,autoencoder.layers[7].output)\n",
    "\n",
    "decoder_input = Input(shape=autoencoder.layers[8].input_shape[1:])\n",
    "decoder = decoder_input\n",
    "for layer in autoencoder.layers[8:]:\n",
    "    decoder = layer(decoder)\n",
    "    \n",
    "decoder_model = Model(decoder_input,decoder)\n",
    "\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfjs.converters.save_keras_model(autoencoder, './tensorflowjs-model/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2, os\n",
    "import numpy as np\n",
    "#import pandas as pd\n",
    "\n",
    "train_dir = \"../Datasets/DIV2K_train_HR/\"\n",
    "valid_dir = \"../Datasets/DIV2K_valid_HR/\"\n",
    "\n",
    "X = list()\n",
    "Y = list()\n",
    "#files = pd.read_csv('image_data.csv').Path\n",
    "train_files = os.listdir(train_dir)\n",
    "valid_files = os.listdir(valid_dir)\n",
    "\n",
    "def getImage(file):\n",
    "    #â€ªC:\\Users\\mayan\\Downloads\\2-dog.jpg\n",
    "    frame = cv2.imread(file,1)\n",
    "    return [np.array([cv2.resize(frame,(in_width,in_height))]),np.array([cv2.resize(frame,(out_width,out_height))])]\n",
    "\n",
    "def getValidation():\n",
    "    x_valid,y_valid = [],[]   \n",
    "    for file in valid_files:\n",
    "        frame = cv2.imread(valid_dir + file,1)\n",
    "        try:\n",
    "            #y_valid.append(cv2.resize(frame,(out_width,out_height)))\n",
    "            x_valid.append(cv2.resize(frame,(in_width,in_height)))\n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "    return np.array(x_valid),np.array(y_valid)\n",
    "\n",
    "\n",
    "for file in train_files:\n",
    "    frame = cv2.imread(train_dir + file,1)\n",
    "    try:\n",
    "        X.append(cv2.resize(frame,(in_width,in_height)))\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "\n",
    "\"\"\"\n",
    "train_dir = \"../Datasets/Text Images/\"\n",
    "train_files = os.listdir(train_dir)\n",
    "\n",
    "for file in train_files:\n",
    "    frame = cv2.imread(train_dir + file,1)\n",
    "    try:\n",
    "        X.append(cv2.resize(frame,(in_width,in_height)))\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "        \n",
    "x_valid,y_valid = getValidation()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files = os.listdir(\"./../Datasets/flickr30k_images/\")\n",
    "for file in train_files:\n",
    "    frame = cv2.imread( './../Datasets/flickr30k_images/'+ file,1)\n",
    "    try:\n",
    "        height, width, channels = frame.shape\n",
    "        if height >= in_height and width >= in_width:\n",
    "            X.append(cv2.resize(frame,(in_width,in_height)))\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "    \n",
    "X = np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(horizontal_flip=True,vertical_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from keras.optimizers import Adam, Adadelta\n",
    "\n",
    "validation_accuracy = 0.7050989866256714\n",
    "learning_rate = (1-validation_accuracy)/10000\n",
    "#autoencoder.compile(optimizer=Adam(lr = learning_rate),  loss='mean_absolute_error',metrics=['acc'])\n",
    "\n",
    "#autoencoder.compile(optimizer=Adam(lr = 2.9490101951523684e-05),  loss='mse',metrics=['acc'])\n",
    "it = datagen.flow(np.array(X),batch_size=512,shuffle = True)\n",
    "\n",
    "x_test = getImage(train_dir+'IMG_20191018_232049.jpg')[0];\n",
    "\n",
    "for i in range(1000):\n",
    "    X_ = next(it)        \n",
    "    hist = autoencoder.fit(X_, X_,\n",
    "                epochs=10,\n",
    "                batch_size=8,validation_data = (x_valid,x_valid))\n",
    "    clear_output() \n",
    "    \n",
    "    decoded_imgs = autoencoder.predict(x_test)\n",
    "    h = autoencoder.evaluate(x_test,x_test)\n",
    "        \n",
    "    print(\"Validation Loss\",np.mean(hist.history['val_loss']))\n",
    "    print(\"Validation Accuracy\",np.mean(hist.history['val_acc']))   \n",
    "    \n",
    "    print(\"Test Loss\",h[0])\n",
    "    print(\"Test Accuracy\",h[1])\n",
    "    \n",
    "    cv2.imwrite(\"./restructured/test_image.jpg\" ,decoded_imgs[0])\n",
    "    cv2.imwrite(\"./restructured/original-test_image.jpg\" ,x_test[0])\n",
    "    \n",
    "    decoder_model.save(\"16C-%dx%d-decoder.h5\"%(out_height,out_width))\n",
    "    encoder_model.save(\"16C-%dx%d-encoder.h5\"%(in_height,in_width))\n",
    "    '''\n",
    "    encoder_model.save_weights('encoder-weights-%dx%d.h5'%(in_height,in_width)) \n",
    "    '''\n",
    "    autoencoder.save_weights('16C-Text--Images-autoencoder-weights-model-13-360x640-360x640.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = datagen.flow(x_test,batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_imgs = next(it)\n",
    "cv2.imshow(\"Reconstructed\", decoded_imgs[0]/255) \n",
    "\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = getImage('C:/Users/mayan/Desktop/tensorflow-autoencoder/static/pexels-photo-257840.jpeg')[0];\n",
    "#x_test = getImage(train_dir + 'IMG_20191018_232049.jpg')[0]\n",
    "#x_test = x_valid[4:5]\n",
    "\n",
    "#decoded_imgs = autoencoder.predict(x_test)\n",
    "import sys\n",
    "\n",
    "decoded_imgs = encoder_model.predict(x_test)\n",
    "\n",
    "decoded_imgs = decoded_imgs/np.max(decoded_imgs) * 255\n",
    "decoded_imgs  = decoded_imgs.astype('uint8')\n",
    "\n",
    "decoded_imgs = decoder_model.predict(decoded_imgs)\n",
    "decoded_imgs = decoded_imgs/np.max(decoded_imgs)\n",
    "\n",
    "h = autoencoder.evaluate(x_test,x_test)\n",
    "\n",
    "print(\"Test Loss\",h[0])\n",
    "print(\"Test Accuracy\",h[1])\n",
    "\n",
    "\n",
    "cv2.imshow(\"Reconstructed\", decoded_imgs[0]) \n",
    "cv2.imshow(\"Original\", x_test[0]) \n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model.save(\"encoder-final-model.h5\")\n",
    "decoder_model.save(\"decoder-final-model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files = os.listdir(\"C:/Users/mayan/Machine Learning/Datasets/Events/\")\n",
    "for file in train_files:\n",
    "    frame = cv2.imread(\"C:/Users/mayan/Machine Learning/Datasets/Events/\" + file,1)\n",
    "    try:\n",
    "        height, width, channels = frame.shape\n",
    "        if height >= in_height and width >= in_width:\n",
    "            X.append(cv2.resize(frame,(in_width,in_height)))\n",
    "    except Exception as ex:\n",
    "        print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = autoencoder.evaluate(x_test,x_test)\n",
    "#autoencoder.metrics_names\n",
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "{'learning_rate': 2.9490101951523684e-05,\n",
    " 'beta_1': 0.8999999761581421,\n",
    " 'beta_2': 0.9990000128746033,\n",
    " 'decay': 0.0,\n",
    " 'epsilon': 1e-07,\n",
    " 'amsgrad': False}\n",
    "\"\"\"\n",
    "\n",
    "autoencoder.optimizer.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Save Model\n",
    "json= decoder_model.to_json()\n",
    "with open(\"decoder.json\", \"w\") as json_file:\n",
    "    json_file.write(json)\n",
    "\n",
    "decoder_model.save_weights(\"decoder.h5\")\n",
    "\n",
    "json= encoder_model.to_json()\n",
    "with open(\"encoder.json\", \"w\") as json_file:\n",
    "    json_file.write(json)\n",
    "encoder_model.save_weights(\"encoder.h5\")\n",
    "\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_test,y_test = next(getData(16))\n",
    "x_valid,y_valid = getImage('C:/Users/mayan/Downloads/2-dog.jpg')\n",
    "#x_test,y_test = getImage('dark-fantasy-wallpapers-28123-6689698.jpg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import imutils, time\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "num_images = 10\n",
    "np.random.seed(42)\n",
    "\n",
    "random_test_images = np.random.randint(x_valid.shape[0], size=min(num_images,x_valid.size))\n",
    "\n",
    "encoded_imgs = encoder_model.predict(x_valid)\n",
    "decoded_imgs = decoder_model.predict(encoded_imgs)\n",
    "\n",
    "#decoded_imgs = autoencoder.predict(x_test)\n",
    "\n",
    "plt.figure(figsize=(60, 50))\n",
    "\n",
    "for i, image_idx in enumerate(random_test_images):\n",
    "    # plot original image\n",
    "    ax = plt.subplot(3, num_images, i + 1)\n",
    "    plt.imshow(x_valid[image_idx])\n",
    "    #plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    cv2.imwrite(\"./real/%s.jpg\" %time.time(),x_valid[image_idx])\n",
    "    '''\n",
    "    # plot encoded image\n",
    "    ax = plt.subplot(3, num_images, num_images + i + 1)\n",
    "    plt.imshow(encoded_imgs[image_idx].reshape(32, 16))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    '''\n",
    "    # plot reconstructed image\n",
    "    ax = plt.subplot(3, num_images, num_images + i + 1)\n",
    "    name = time.time()\n",
    "    cv2.imwrite(\"./restructured/%s.jpg\" %name,decoded_imgs[image_idx] )\n",
    "    cv2.imwrite(\"./restructured/original-%s.jpg\" %name,x_valid[image_idx] )\n",
    "    plt.imshow(decoded_imgs[image_idx]/255)\n",
    "    #plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
