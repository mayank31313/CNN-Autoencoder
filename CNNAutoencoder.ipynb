{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, Conv2D, Flatten, MaxPooling2D, Reshape, UpSampling2D, BatchNormalization, AveragePooling2D, Activation, Lambda, Conv2DTranspose, ReLU\n",
    "from keras.models import Model, Sequential\n",
    "from keras.regularizers import l1\n",
    "\n",
    "in_height = 720//2\n",
    "in_width =  1280//2\n",
    "\n",
    "#out_height = 720\n",
    "#out_width = 1280\n",
    "\n",
    "out_height = in_height\n",
    "out_width = in_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 360, 640, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 360, 640, 8)       224       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 360, 640, 8)       0         \n",
      "_________________________________________________________________\n",
      "average_pooling2d_1 (Average (None, 180, 320, 8)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 180, 320, 16)      1168      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 180, 320, 16)      0         \n",
      "_________________________________________________________________\n",
      "average_pooling2d_2 (Average (None, 90, 160, 16)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 90, 160, 32)       4640      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 90, 160, 32)       0         \n",
      "_________________________________________________________________\n",
      "average_pooling2d_3 (Average (None, 45, 80, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 115200)            0         \n",
      "=================================================================\n",
      "Total params: 6,032\n",
      "Trainable params: 6,032\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 115200)            0         \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 45, 80, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 90, 160, 16)       4624      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 90, 160, 16)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 180, 320, 16)      2320      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 180, 320, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 360, 640, 8)       1160      \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 360, 640, 3)       27        \n",
      "=================================================================\n",
      "Total params: 8,131\n",
      "Trainable params: 8,131\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.constraints import maxnorm\n",
    "from keras.losses import *\n",
    "encoder_input =  Input(shape=(in_height,in_width,3))\n",
    "\n",
    "max_norm = 3\n",
    "# Encoder Layers\n",
    "encoder = Conv2D(8, (3, 3),padding='same', kernel_constraint=maxnorm(max_norm))(encoder_input)\n",
    "encoder = Activation('relu')(encoder)\n",
    "encoder = AveragePooling2D((2, 2), padding='same',)(encoder)\n",
    "encoder = Conv2D(16, (3, 3), padding='same', kernel_constraint=maxnorm(max_norm))(encoder)\n",
    "encoder = Activation('relu')(encoder)\n",
    "encoder = AveragePooling2D((2, 2), padding='same')(encoder)\n",
    "\n",
    "encoder = Conv2D(32, (3, 3), padding='same', kernel_constraint=maxnorm(max_norm))(encoder)\n",
    "encoder = Activation('relu')(encoder)\n",
    "encoder = AveragePooling2D((2, 2), padding='same')(encoder)\n",
    "\n",
    "encoder =  Flatten(name='flatten_1')(encoder)\n",
    "\n",
    "encoder_model = Model(encoder_input,encoder)\n",
    "encoder_model.summary()\n",
    "\n",
    "#encoder_model.load_weights(\"16C-360x640-encoder.h5\")\n",
    "\n",
    "# Decoder Layers\n",
    "decoder_input = Input(shape=(45 * 80 * 32,))\n",
    "decoder = Reshape(( 45, 80, 32))(decoder_input)\n",
    "decoder = Conv2DTranspose(16, (3, 3), padding='same',strides=2, kernel_constraint=maxnorm(max_norm))(decoder)\n",
    "decoder = Activation('relu')(decoder)\n",
    "decoder = Conv2DTranspose(16, (3, 3), padding='same',strides=2, kernel_constraint=maxnorm(max_norm))(decoder)\n",
    "decoder = Activation('relu')(decoder)\n",
    "decoder = Conv2DTranspose(8, (3, 3), padding='same',strides=2, kernel_constraint=maxnorm(max_norm))(decoder)\n",
    "decoder = Conv2D(3, (1, 1), padding='same')(decoder)\n",
    "\n",
    "#decoder = ReLU(max_value=255)(decoder)\n",
    "decoder_model = Model(decoder_input,decoder)\n",
    "decoder_model.summary()\n",
    "\n",
    "autoencoder = Model(encoder_input,decoder_model(encoder))\n",
    "autoencoder.compile(optimizer='adam',  loss=logcosh,metrics=['acc'])\n",
    "\n",
    "autoencoder.load_weights('weights-improvement-01-0.87.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mayan\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py:341: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_11 (InputLayer)        (None, 360, 640, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 360, 640, 8)       224       \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 360, 640, 8)       0         \n",
      "_________________________________________________________________\n",
      "average_pooling2d_16 (Averag (None, 180, 320, 8)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 180, 320, 16)      1168      \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 180, 320, 16)      0         \n",
      "_________________________________________________________________\n",
      "average_pooling2d_17 (Averag (None, 90, 160, 16)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 90, 160, 16)       2320      \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 90, 160, 16)       0         \n",
      "_________________________________________________________________\n",
      "average_pooling2d_18 (Averag (None, 45, 80, 16)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 57600)             0         \n",
      "_________________________________________________________________\n",
      "model_13 (Model)             (None, 360, 640, 3)       5827      \n",
      "=================================================================\n",
      "Total params: 9,539\n",
      "Trainable params: 9,539\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "encoder_model = load_model(\"16C-360x640-encoder.h5\")\n",
    "decoder_model = load_model(\"16C-360x640-decoder.h5\")\n",
    "\n",
    "autoencoder = Model(encoder_model.inputs,decoder_model(encoder_model.outputs))\n",
    "autoencoder.summary()\n",
    "\n",
    "autoencoder.compile(optimizer='adam',  loss=logcosh,metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfjs.converters.save_keras_model(autoencoder, './tensorflowjs-model/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2, os\n",
    "import numpy as np\n",
    "#import pandas as pd\n",
    "\n",
    "train_dir = \"../Datasets/DIV2K_train_HR/\"\n",
    "valid_dir = \"../Datasets/DIV2K_valid_HR/\"\n",
    "\n",
    "X = list()\n",
    "Y = list()\n",
    "#files = pd.read_csv('image_data.csv').Path\n",
    "train_files = os.listdir(train_dir)\n",
    "valid_files = os.listdir(valid_dir)\n",
    "\n",
    "def getImage(file):\n",
    "    #â€ªC:\\Users\\mayan\\Downloads\\2-dog.jpg\n",
    "    frame = cv2.imread(file,1)\n",
    "    return [np.array([cv2.resize(frame,(in_width,in_height))]),np.array([cv2.resize(frame,(out_width,out_height))])]\n",
    "\n",
    "def getValidation():\n",
    "    x_valid,y_valid = [],[]   \n",
    "    for file in valid_files:\n",
    "        frame = cv2.imread(valid_dir + file,1)\n",
    "        try:\n",
    "            #y_valid.append(cv2.resize(frame,(out_width,out_height)))\n",
    "            x_valid.append(cv2.resize(frame,(in_width,in_height)))\n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "    return np.array(x_valid),np.array(y_valid)\n",
    "\n",
    "\"\"\"\n",
    "for file in train_files:\n",
    "    frame = cv2.imread(train_dir + file,1)\n",
    "    try:\n",
    "        X.append(cv2.resize(frame,(in_width,in_height)))\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "\"\"\"\n",
    "x_valid,y_valid = getValidation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_output = encoder_model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"images.npy\",X)\n",
    "np.save(\"encoded_output.npy\",encoded_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'shape'\n",
      "'NoneType' object has no attribute 'shape'\n"
     ]
    }
   ],
   "source": [
    "train_files = os.listdir(\"./../Datasets/flickr30k_images/\")\n",
    "for file in train_files:\n",
    "    frame = cv2.imread( './../Datasets/flickr30k_images/'+ file,1)\n",
    "    try:\n",
    "        height, width, channels = frame.shape\n",
    "        if height >= in_height and width >= in_width:\n",
    "            X.append(cv2.resize(frame,(in_width,in_height)))\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "    \n",
    "X = np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7078 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "datagen = ImageDataGenerator(horizontal_flip=True,vertical_flip=True)\n",
    "train_generator = datagen.flow_from_directory(\n",
    "        'D:\\\\Fast and Furious',\n",
    "        target_size=(in_height, in_width),\n",
    "        batch_size=1,\n",
    "        shuffle=True,\n",
    "        class_mode=\"input\")\n",
    "\n",
    "filepath=\"weights-improvement-{epoch:02d}-{val_acc:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 12ms/step\n",
      "Test Loss 2.6306862831115723\n",
      "Test Accuracy 0.9407768845558167\n",
      "Epoch 1/1\n",
      "24757/28312 [=========================>....] - ETA: 2:14 - loss: 3.9653 - acc: 0.8881"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "from keras.optimizers import Adam, Adadelta\n",
    "\n",
    "validation_accuracy = 0.7050989866256714\n",
    "learning_rate = (1-validation_accuracy)/10000\n",
    "\n",
    "#it = datagen.flow(np.array(X),batch_size=512,shuffle = True)\n",
    "\n",
    "x_test = getImage('C:/Users/mayan/Desktop/tensorflow-autoencoder/static/pexels-photo-257840.jpeg')[0];\n",
    "\n",
    "while True:\n",
    "    \"\"\"\n",
    "    X_ = next(train_generator)[0]  \n",
    "    hist = autoencoder.fit(X_,\n",
    "                epochs=10,\n",
    "                batch_size=8,validation_data = (x_valid,x_valid))\n",
    "    \"\"\"\n",
    "    autoencoder.fit_generator(train_generator,steps_per_epoch=7078 * 4,validation_data=(x_valid,x_valid),epochs=1,callbacks=[checkpoint])\n",
    "    clear_output()\n",
    "    decoded_imgs = autoencoder.predict(x_test)\n",
    "    h = autoencoder.evaluate(x_test,x_test)\n",
    "    \n",
    "    print(\"Test Loss\",h[0])\n",
    "    print(\"Test Accuracy\",h[1])\n",
    "    \n",
    "    cv2.imwrite(\"./restructured/test_image.jpg\" ,decoded_imgs[0])\n",
    "    cv2.imwrite(\"./restructured/original-test_image.jpg\" ,x_test[0])\n",
    "    \n",
    "    decoder_model.save(\"32C-%dx%d-decoder.h5\"%(out_height,out_width))\n",
    "    encoder_model.save(\"32C-%dx%d-encoder.h5\"%(in_height,in_width))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = datagen.flow(x_test,batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_imgs = next(train_generator)\n",
    "cv2.imshow(\"Desired\", decoded_imgs[1][0]/255) \n",
    "cv2.imshow(\"Reconstructed\", decoded_imgs[0][0]/255) \n",
    "\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = getImage('C:/Users/mayan/Desktop/tensorflow-autoencoder/static/pexels-photo-257840.jpeg')[0];\n",
    "#x_test = getImage(train_dir + 'IMG_20191018_232049.jpg')[0]\n",
    "#x_test = x_valid[4:5]\n",
    "\n",
    "#decoded_imgs = autoencoder.predict(x_test)\n",
    "import sys\n",
    "\n",
    "decoded_imgs = encoder_model.predict(x_test)\n",
    "decoded_imgs = decoded_imgs/np.max(decoded_imgs) * 255\n",
    "decoded_imgs  = decoded_imgs.astype('uint8')\n",
    "\n",
    "decoded_imgs = decoder_model.predict(decoded_imgs)\n",
    "decoded_imgs = decoded_imgs/np.max(decoded_imgs)\n",
    "\n",
    "\n",
    "cv2.imshow(\"Reconstructed\", decoded_imgs[0]) \n",
    "cv2.imshow(\"Original\", x_test[0]) \n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model.save(\"encoder-final-model.h5\")\n",
    "decoder_model.save(\"decoder-final-model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files = os.listdir(\"C:/Users/mayan/Machine Learning/Datasets/Events/\")\n",
    "for file in train_files:\n",
    "    frame = cv2.imread(\"C:/Users/mayan/Machine Learning/Datasets/Events/\" + file,1)\n",
    "    try:\n",
    "        height, width, channels = frame.shape\n",
    "        if height >= in_height and width >= in_width:\n",
    "            X.append(cv2.resize(frame,(in_width,in_height)))\n",
    "    except Exception as ex:\n",
    "        print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = autoencoder.evaluate(x_test,x_test)\n",
    "#autoencoder.metrics_names\n",
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "{'learning_rate': 2.9490101951523684e-05,\n",
    " 'beta_1': 0.8999999761581421,\n",
    " 'beta_2': 0.9990000128746033,\n",
    " 'decay': 0.0,\n",
    " 'epsilon': 1e-07,\n",
    " 'amsgrad': False}\n",
    "\"\"\"\n",
    "\n",
    "autoencoder.optimizer.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Save Model\n",
    "json= decoder_model.to_json()\n",
    "with open(\"decoder.json\", \"w\") as json_file:\n",
    "    json_file.write(json)\n",
    "\n",
    "decoder_model.save_weights(\"decoder.h5\")\n",
    "\n",
    "json= encoder_model.to_json()\n",
    "with open(\"encoder.json\", \"w\") as json_file:\n",
    "    json_file.write(json)\n",
    "encoder_model.save_weights(\"encoder.h5\")\n",
    "\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_test,y_test = next(getData(16))\n",
    "x_valid,y_valid = getImage('C:/Users/mayan/Downloads/2-dog.jpg')\n",
    "#x_test,y_test = getImage('dark-fantasy-wallpapers-28123-6689698.jpg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import imutils, time\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "num_images = 10\n",
    "np.random.seed(42)\n",
    "\n",
    "random_test_images = np.random.randint(x_valid.shape[0], size=min(num_images,x_valid.size))\n",
    "\n",
    "encoded_imgs = encoder_model.predict(x_valid)\n",
    "decoded_imgs = decoder_model.predict(encoded_imgs)\n",
    "\n",
    "#decoded_imgs = autoencoder.predict(x_test)\n",
    "\n",
    "plt.figure(figsize=(60, 50))\n",
    "\n",
    "for i, image_idx in enumerate(random_test_images):\n",
    "    # plot original image\n",
    "    ax = plt.subplot(3, num_images, i + 1)\n",
    "    plt.imshow(x_valid[image_idx])\n",
    "    #plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    cv2.imwrite(\"./real/%s.jpg\" %time.time(),x_valid[image_idx])\n",
    "    '''\n",
    "    # plot encoded image\n",
    "    ax = plt.subplot(3, num_images, num_images + i + 1)\n",
    "    plt.imshow(encoded_imgs[image_idx].reshape(32, 16))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    '''\n",
    "    # plot reconstructed image\n",
    "    ax = plt.subplot(3, num_images, num_images + i + 1)\n",
    "    name = time.time()\n",
    "    cv2.imwrite(\"./restructured/%s.jpg\" %name,decoded_imgs[image_idx] )\n",
    "    cv2.imwrite(\"./restructured/original-%s.jpg\" %name,x_valid[image_idx] )\n",
    "    plt.imshow(decoded_imgs[image_idx]/255)\n",
    "    #plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
